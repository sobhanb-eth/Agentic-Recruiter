{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":121144,"databundleVersionId":14484960,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sobhanbahrami0/agentic-recruiter?scriptVersionId=282957160\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"#Kaggle Notebook: Multi-Agent Resume Screener with Memory, Tools & Evaluation\n# - Built with Google Agent Development Kit (ADK)\n# - Scenario: hiring manager uploads Job Description + resumes (PDF/DOCX/URL)\n# - Agent team:\n#     * JD Interpreter Agent\n#     * Resume Extractor & Normalizer Agent (LoopAgent over multiple CVs)\n#     * Matching & Ranking Agent\n#     * Explanation / Social Summaries Agent\n# - Features:\n#     * Multi-agent (SequentialAgent + LoopAgent orchestrated by a root agent)\n#     * Tools: custom tools, Google Search, placeholders for MCP + OpenAPI tools\n#     * Sessions: InMemory + DatabaseSessionService (+ VertexAiSessionService optional)\n#     * Long-term memory: VertexAiMemoryBankService (preferences, past roles)\n#     * Context engineering: EventsCompactionConfig for summarised histories\n#     * Observability: LoggingPlugin\n#     * Agent evaluation: evalset + config + `adk eval` call\n#     * A2A / Deployment: optional section for turning this into an HTTP agent\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Setup: Imports & API keys\n# ============================================================================\n# IMPORTANT: Set up Google Cloud credentials BEFORE importing ADK!\n# This ensures ADK uses Vertex AI with proper OAuth authentication.\n# ============================================================================\n\nimport os\nimport logging\nimport vertexai\nfrom typing import Any, Dict, List\nfrom kaggle_secrets import UserSecretsClient\n\n# --- Set up Google Cloud credentials FIRST ---\nuser_secrets = UserSecretsClient()\n\n# Get and set the GCloud credential (OAuth) - this is key!\nuser_credential = user_secrets.get_gcloud_credential()\nuser_secrets.set_tensorflow_credential(user_credential)\nprint(\"‚úÖ Google Cloud OAuth credentials configured.\")\n\n# --- Now import ADK (after credentials are set) ---\nfrom google.adk.agents import Agent, LlmAgent, SequentialAgent, LoopAgent, ParallelAgent\nfrom google.adk.models.google_llm import Gemini\nfrom google.adk.runners import Runner, InMemoryRunner\nfrom google.adk.sessions import (\n    InMemorySessionService,\n    DatabaseSessionService,\n    VertexAiSessionService,\n)\nfrom google.adk.tools.tool_context import ToolContext\n\n# App and Context Compaction (Day 3 - Context Engineering)\nfrom google.adk.apps.app import App, EventsCompactionConfig\n\n# Observability (Day 4 - LoggingPlugin)\nfrom google.adk.plugins.logging_plugin import LoggingPlugin\n\n# Memory services\nfrom google.adk.memory import (\n    InMemoryMemoryService,\n    VertexAiMemoryBankService,\n)\n\n# Built-in tools\nfrom google.adk.tools.google_search_tool import google_search\nfrom google.adk.tools import load_memory, preload_memory\nfrom google.adk.tools import FunctionTool, AgentTool\n\n# For API types (retry config, etc.)\nfrom google.genai import types\n\nprint(\"‚úÖ ADK components imported successfully.\")\nprint(\"   - App & EventsCompactionConfig (Context Engineering)\")\nprint(\"   - LoggingPlugin (Observability)\")\nprint(\"   - SequentialAgent, LoopAgent, ParallelAgent (Workflow Agents)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T18:51:23.390014Z","iopub.execute_input":"2025-11-30T18:51:23.390321Z","iopub.status.idle":"2025-11-30T18:51:47.752301Z","shell.execute_reply.started":"2025-11-30T18:51:23.390298Z","shell.execute_reply":"2025-11-30T18:51:47.751106Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Global Config & Logging\n# ============================================================================\n# Using Vertex AI with Google Cloud OAuth credentials (from Kaggle SDK add-on)\n# ============================================================================\n\n# Models ‚Äì use whatever is suggested in the Kaggle course\nMODEL_NAME_FAST = \"gemini-2.5-flash-lite\"\nMODEL_NAME_STRONG = \"gemini-2.5-flash\"\n\nAPP_NAME = \"resume-matcher-app\"\nUSER_ID = \"demo_recruiter\"\n\n# --- Load Vertex AI configuration ---\nPROJECT_ID = user_secrets.get_secret(\"VERTEX_PROJECT_ID\")\nLOCATION = user_secrets.get_secret(\"VERTEX_LOCATION\")\nprint(f\"‚úÖ Vertex AI config: Project={PROJECT_ID}, Location={LOCATION}\")\n\n# Set environment variables for ADK\nos.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\nos.environ[\"GOOGLE_CLOUD_LOCATION\"] = LOCATION\n\n# Also load API key (as backup / for some tools)\nGOOGLE_API_KEY = user_secrets.get_secret(\"GOOGLE_API_KEY\")\nos.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\nprint(\"‚úÖ GOOGLE_API_KEY loaded.\")\n\n# --- Initialize Vertex AI ---\nvertexai.init(project=PROJECT_ID, location=LOCATION)\nprint(\"‚úÖ Vertex AI initialized with OAuth credentials.\")\n\n# --- Logging ---\nlogging.basicConfig(\n    filename=\"resume_agent.log\",\n    level=logging.DEBUG,\n    format=\"%(filename)s:%(lineno)s %(levelname)s:%(message)s\",\n)\nprint(\"‚úÖ Logging configured -> resume_agent.log\")\n\n# --- Retry config (same style as course notebooks) ---\nretry_config = types.HttpRetryOptions(\n    attempts=5,\n    exp_base=7,\n    initial_delay=1,\n    http_status_codes=[429, 500, 503, 504],\n)\nprint(\"‚úÖ Retry config set.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T18:51:53.168127Z","iopub.execute_input":"2025-11-30T18:51:53.168696Z","iopub.status.idle":"2025-11-30T18:51:53.605247Z","shell.execute_reply.started":"2025-11-30T18:51:53.16867Z","shell.execute_reply":"2025-11-30T18:51:53.604207Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.core.display import display, HTML\nfrom jupyter_server.serverapp import list_running_servers\n\ndef get_adk_proxy_url():\n    PROXY_HOST = \"https://kkb-production.jupyter-proxy.kaggle.net\"\n    ADK_PORT = \"8000\"\n\n    servers = list(list_running_servers())\n    if not servers:\n        raise Exception(\"No running Jupyter servers found.\")\n\n    baseURL = servers[0][\"base_url\"]\n    \n    # Extract kernel and token\n    try:\n        path_parts = baseURL.split(\"/\")\n        kernel = path_parts[2]\n        token = path_parts[3]\n    except IndexError:\n        raise Exception(f\"Could not parse kernel/token from base URL: {baseURL}\")\n\n    url_prefix = f\"/k/{kernel}/{token}/proxy/proxy/{ADK_PORT}\"\n    url = f\"{PROXY_HOST}{url_prefix}\"\n\n    styled_html = f\"\"\"\n    <div style=\"padding: 15px; border: 2px solid #f0ad4e; border-radius: 8px; background-color: #fef9f0; margin: 20px 0;\">\n        <h3>üöÄ ADK Web UI is Ready</h3>\n        <p>1. Run the cell below (`!adk web ...`) to start the server.</p>\n        <p>2. Wait for it to say \"Uvicorn running on...\"</p>\n        <p>3. <strong><a href='{url}' target='_blank' style=\"background-color: #1a73e8; color: white; padding: 5px 10px; text-decoration: none; border-radius: 5px;\">CLICK HERE TO OPEN UI</a></strong></p>\n    </div>\n    \"\"\"\n    display(HTML(styled_html))\n    return url_prefix\n\n# Display the button\nurl_prefix = get_adk_proxy_url()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T18:51:56.904617Z","iopub.execute_input":"2025-11-30T18:51:56.90495Z","iopub.status.idle":"2025-11-30T18:51:57.098031Z","shell.execute_reply.started":"2025-11-30T18:51:56.904926Z","shell.execute_reply":"2025-11-30T18:51:57.097204Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Session & Memory Configuration\n# ============================================================================\n# Memory Architecture for the Multi-Agent Resume Screener:\n#\n# üß† Short-term Memory (Sessions):\n#    - DatabaseSessionService (SQLite) for persistent conversation storage\n#    - Each screening session maintains its own conversation history\n#\n# üß† Long-term Memory:\n#    - Development: InMemoryMemoryService (keyword-based, resets on restart)\n#    - Production: VertexAiMemoryBankService (memory consolidation, semantic search)\n#\n# For production deployment, use: adk deploy agent_engine\n# Memory Bank is automatically included with Agent Engine!\n# ============================================================================\n\n# --- Session Service (Short-term Memory) ---\n# SQLite persists conversations between notebook restarts\n\ndb_file_path = \"/kaggle/working/resume_agent_sessions.db\"\nif os.path.exists(db_file_path):\n    try:\n        os.remove(db_file_path)\n        print(\"üßπ Cleaned up old database file.\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Could not remove DB file: {e}\")\n\nsession_service = DatabaseSessionService(\n    db_url=f\"sqlite:///{db_file_path}\"\n)\nprint(f\"‚úÖ DatabaseSessionService for sessions initialised {db_file_path}.\")\n\n# --- Memory Service (Long-term Memory) ---\n# For Kaggle notebook: InMemoryMemoryService provides the core memory workflow\n# It uses keyword matching and stores raw events (no memory consolidation)\nmemory_service = InMemoryMemoryService()\nprint(\"‚úÖ InMemoryMemoryService initialised.\")\n\n# --- Production Configuration (for Agent Engine deployment) ---\n# When you deploy with `adk deploy agent_engine`, Memory Bank is automatic!\n# Here's the memory bank configuration for resume screening:\n\nMEMORY_BANK_CONFIG = {\n    \"customization_configs\": [\n        {\n            \"memory_topics\": [\n                # Built-in managed topics\n                {\"managed_memory_topic\": {\"managed_topic_enum\": \"USER_PREFERENCES\"}},\n                {\"managed_memory_topic\": {\"managed_topic_enum\": \"KEY_CONVERSATION_DETAILS\"}},\n                {\"managed_memory_topic\": {\"managed_topic_enum\": \"EXPLICIT_INSTRUCTIONS\"}},\n                # Custom topics for resume screening\n                {\n                    \"custom_memory_topic\": {\n                        \"label\": \"job_requirements\",\n                        \"description\": \"Key job requirements, skills, qualifications from JDs.\"\n                    }\n                },\n                {\n                    \"custom_memory_topic\": {\n                        \"label\": \"candidate_profiles\",\n                        \"description\": \"Candidate skills, experience, education from resumes.\"\n                    }\n                },\n                {\n                    \"custom_memory_topic\": {\n                        \"label\": \"screening_decisions\",\n                        \"description\": \"Match scores, strengths, concerns for candidates.\"\n                    }\n                }\n            ]\n        }\n    ]\n}\n\nprint(\"\\nüìù Memory Configuration Summary:\")\nprint(f\"   Session Service: {type(session_service).__name__} (persistent SQLite)\")\nprint(f\"   Memory Service:  {type(memory_service).__name__} (development mode)\")\nprint(\"\\nüí° For production with Vertex AI Memory Bank:\")\nprint(\"   1. Create agent code in a directory (e.g., resume_agent/)\")\nprint(\"   2. Run: adk deploy agent_engine --project=$PROJECT_ID --region=$LOCATION resume_agent/\")\nprint(\"   3. Memory Bank is automatically enabled with LLM consolidation + semantic search!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T18:52:00.275775Z","iopub.execute_input":"2025-11-30T18:52:00.276323Z","iopub.status.idle":"2025-11-30T18:52:00.377878Z","shell.execute_reply.started":"2025-11-30T18:52:00.276299Z","shell.execute_reply":"2025-11-30T18:52:00.377025Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%pip install -q PyPDF2 python-docx","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Custom Tools for Document Fetching and Parsing\n# ============================================================================\nimport requests\nimport re\nimport json\nfrom io import BytesIO\nfrom typing import Optional\n\n# For PDF parsing\ntry:\n    import PyPDF2\n    HAS_PYPDF2 = True\nexcept ImportError:\n    HAS_PYPDF2 = False\n    print(\"‚ö†Ô∏è PyPDF2 not available - PDF extraction will use fallback\")\n\n# For DOCX parsing\ntry:\n    from docx import Document as DocxDocument\n    HAS_DOCX = True\nexcept ImportError:\n    HAS_DOCX = False\n    print(\"‚ö†Ô∏è python-docx not available - DOCX extraction will use fallback\")\n\n\ndef fetch_url_text(url: str) -> dict:\n    \"\"\"\n    Fetch text content from a URL (webpage, raw text, or API endpoint).\n    \n    Args:\n        url: The URL to fetch content from\n    \n    Returns:\n        dict with 'status' and 'content' or 'error'\n    \"\"\"\n    try:\n        headers = {\n            'User-Agent': 'Mozilla/5.0 (compatible; AgenticRecruiter/1.0)'\n        }\n        response = requests.get(url, headers=headers, timeout=30)\n        response.raise_for_status()\n        \n        content_type = response.headers.get('content-type', '').lower()\n        \n        if 'text/html' in content_type:\n            # Basic HTML to text conversion (strip tags)\n            text = re.sub(r'<script[^>]*>.*?</script>', '', response.text, flags=re.DOTALL)\n            text = re.sub(r'<style[^>]*>.*?</style>', '', text, flags=re.DOTALL)\n            text = re.sub(r'<[^>]+>', ' ', text)\n            text = re.sub(r'\\s+', ' ', text).strip()\n            return {'status': 'success', 'content': text[:15000]}  # Limit size\n        else:\n            return {'status': 'success', 'content': response.text[:15000]}\n            \n    except requests.RequestException as e:\n        return {'status': 'error', 'error': f'Failed to fetch URL: {str(e)}'}\n\n\ndef extract_pdf_from_url(url: str) -> dict:\n    \"\"\"\n    Download a PDF from URL and extract its text content.\n    \n    Args:\n        url: URL pointing to a PDF file\n    \n    Returns:\n        dict with 'status' and 'content' or 'error'\n    \"\"\"\n    try:\n        headers = {'User-Agent': 'Mozilla/5.0 (compatible; AgenticRecruiter/1.0)'}\n        response = requests.get(url, headers=headers, timeout=60)\n        response.raise_for_status()\n        \n        if HAS_PYPDF2:\n            pdf_file = BytesIO(response.content)\n            reader = PyPDF2.PdfReader(pdf_file)\n            text_parts = []\n            for page in reader.pages:\n                text_parts.append(page.extract_text() or '')\n            text = '\\n'.join(text_parts)\n            return {'status': 'success', 'content': text[:15000], 'pages': len(reader.pages)}\n        else:\n            return {'status': 'error', 'error': 'PyPDF2 not installed. Install with: pip install PyPDF2'}\n            \n    except Exception as e:\n        return {'status': 'error', 'error': f'Failed to extract PDF: {str(e)}'}\n\n\ndef extract_docx_from_url(url: str) -> dict:\n    \"\"\"\n    Download a DOCX file from URL and extract its text content.\n    \n    Args:\n        url: URL pointing to a DOCX file\n    \n    Returns:\n        dict with 'status' and 'content' or 'error'\n    \"\"\"\n    try:\n        headers = {'User-Agent': 'Mozilla/5.0 (compatible; AgenticRecruiter/1.0)'}\n        response = requests.get(url, headers=headers, timeout=60)\n        response.raise_for_status()\n        \n        if HAS_DOCX:\n            docx_file = BytesIO(response.content)\n            doc = DocxDocument(docx_file)\n            text_parts = [para.text for para in doc.paragraphs if para.text.strip()]\n            text = '\\n'.join(text_parts)\n            return {'status': 'success', 'content': text[:15000], 'paragraphs': len(text_parts)}\n        else:\n            return {'status': 'error', 'error': 'python-docx not installed. Install with: pip install python-docx'}\n            \n    except Exception as e:\n        return {'status': 'error', 'error': f'Failed to extract DOCX: {str(e)}'}\n\n\ndef parse_document(text_or_url: str) -> dict:\n    \"\"\"\n    Smart document parser - handles raw text, URLs, PDFs, and DOCX files.\n    \n    Args:\n        text_or_url: Either raw text content OR a URL to fetch from\n    \n    Returns:\n        dict with 'status', 'content', and 'source_type'\n    \"\"\"\n    text_or_url = text_or_url.strip()\n    \n    # Check if it's a URL\n    if text_or_url.startswith(('http://', 'https://')):\n        url_lower = text_or_url.lower()\n        \n        if url_lower.endswith('.pdf'):\n            result = extract_pdf_from_url(text_or_url)\n            result['source_type'] = 'pdf_url'\n            return result\n        elif url_lower.endswith('.docx'):\n            result = extract_docx_from_url(text_or_url)\n            result['source_type'] = 'docx_url'\n            return result\n        else:\n            result = fetch_url_text(text_or_url)\n            result['source_type'] = 'webpage'\n            return result\n    else:\n        # It's raw text\n        return {\n            'status': 'success',\n            'content': text_or_url[:15000],\n            'source_type': 'raw_text'\n        }\n\n\nprint(\"‚úÖ Document parsing tools defined:\")\nprint(\"   - fetch_url_text(url)\")\nprint(\"   - extract_pdf_from_url(url)\")\nprint(\"   - extract_docx_from_url(url)\")\nprint(\"   - parse_document(text_or_url) - smart router\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T18:52:05.131804Z","iopub.execute_input":"2025-11-30T18:52:05.132099Z","iopub.status.idle":"2025-11-30T18:52:05.153439Z","shell.execute_reply.started":"2025-11-30T18:52:05.132079Z","shell.execute_reply":"2025-11-30T18:52:05.152622Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Structured Data Extraction Tools\n# ============================================================================\n# These tools help agents extract and normalize structured data\n\ndef extract_job_profile(jd_text: str) -> dict:\n    \"\"\"\n    Parse a job description into a structured profile.\n    This is a tool that the JD Analyst Agent will use.\n    The LLM fills in the structure; this tool provides the schema.\n    \n    Args:\n        jd_text: Raw job description text\n    \n    Returns:\n        dict with structured job profile schema (to be filled by LLM)\n    \"\"\"\n    # Return schema for the LLM to fill\n    return {\n        'instruction': 'Extract the following fields from the job description',\n        'schema': {\n            'title': 'Job title',\n            'level': 'Seniority level (Junior/Mid/Senior/Lead/Principal/Director)',\n            'location': 'Work location or Remote',\n            'experience_min_years': 'Minimum years of experience required',\n            'experience_max_years': 'Maximum years of experience (if mentioned)',\n            'must_have_skills': ['List of required/must-have skills'],\n            'nice_to_have_skills': ['List of preferred/nice-to-have skills'],\n            'responsibilities': ['Key job responsibilities'],\n            'domains': ['Industry domains mentioned (e.g., fintech, healthcare)'],\n            'education': 'Education requirements',\n            'languages': ['Required languages'],\n            'hard_constraints': ['Non-negotiable requirements (visa, clearance, etc.)'],\n            'company_culture': 'Any culture/values mentioned',\n            'compensation': 'Salary range if mentioned'\n        },\n        'jd_text_preview': jd_text[:500] + '...' if len(jd_text) > 500 else jd_text\n    }\n\n\ndef extract_candidate_profile(resume_text: str) -> dict:\n    \"\"\"\n    Parse a resume into a structured candidate profile.\n    This is a tool that the Candidate Profiler Agent will use.\n    \n    Args:\n        resume_text: Raw resume text\n    \n    Returns:\n        dict with structured candidate profile schema (to be filled by LLM)\n    \"\"\"\n    return {\n        'instruction': 'Extract the following fields from the resume',\n        'schema': {\n            'name': 'Candidate name',\n            'email': 'Email address',\n            'phone': 'Phone number',\n            'location': 'Current location',\n            'total_years_experience': 'Total years of professional experience',\n            'current_title': 'Current or most recent job title',\n            'current_company': 'Current or most recent company',\n            'work_history': [\n                {\n                    'title': 'Job title',\n                    'company': 'Company name',\n                    'duration': 'Time period',\n                    'highlights': ['Key achievements']\n                }\n            ],\n            'primary_skills': ['Core technical skills with proficiency'],\n            'secondary_skills': ['Additional skills'],\n            'domains': ['Industries/domains worked in'],\n            'education': [\n                {'degree': 'Degree name', 'institution': 'School', 'year': 'Graduation year'}\n            ],\n            'certifications': ['Professional certifications'],\n            'languages': ['Languages spoken'],\n            'seniority_estimate': 'Estimated seniority level',\n            'strengths': ['Key strengths based on experience'],\n            'potential_concerns': ['Any gaps or concerns to note']\n        },\n        'resume_text_preview': resume_text[:500] + '...' if len(resume_text) > 500 else resume_text\n    }\n\n\ndef compute_fit_score(job_profile: dict, candidate_profile: dict) -> dict:\n    \"\"\"\n    Compute a structured fit score between a job and candidate.\n    This provides the rubric for the Fit Scorer Agent.\n    \n    Args:\n        job_profile: Structured job requirements\n        candidate_profile: Structured candidate profile\n    \n    Returns:\n        dict with scoring rubric\n    \"\"\"\n    return {\n        'instruction': 'Score the candidate against the job requirements',\n        'scoring_dimensions': {\n            'skills_match': {\n                'weight': 0.30,\n                'description': 'How well do candidate skills match must-have requirements?',\n                'score_range': '0-100'\n            },\n            'experience_match': {\n                'weight': 0.25,\n                'description': 'Does experience level match the role requirements?',\n                'score_range': '0-100'\n            },\n            'domain_match': {\n                'weight': 0.20,\n                'description': 'How relevant is their industry/domain experience?',\n                'score_range': '0-100'\n            },\n            'seniority_match': {\n                'weight': 0.15,\n                'description': 'Does seniority level align with the role?',\n                'score_range': '0-100'\n            },\n            'nice_to_haves': {\n                'weight': 0.10,\n                'description': 'How many nice-to-have skills are present?',\n                'score_range': '0-100'\n            }\n        },\n        'output_format': {\n            'overall_score': 'Weighted average 0-100',\n            'fit_label': 'Strong Fit (75+) / Mixed Fit (50-74) / Weak Fit (<50)',\n            'dimension_scores': 'Individual scores per dimension',\n            'evidence_table': 'Job requirement -> Resume evidence mapping',\n            'gaps': 'Missing requirements',\n            'strengths': 'Exceeds requirements',\n            'risks': 'Potential concerns'\n        }\n    }\n\n\nprint(\"‚úÖ Structured extraction tools defined:\")\nprint(\"   - extract_job_profile(jd_text)\")\nprint(\"   - extract_candidate_profile(resume_text)\")\nprint(\"   - compute_fit_score(job_profile, candidate_profile)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T18:52:14.238347Z","iopub.execute_input":"2025-11-30T18:52:14.238686Z","iopub.status.idle":"2025-11-30T18:52:14.25217Z","shell.execute_reply.started":"2025-11-30T18:52:14.238662Z","shell.execute_reply":"2025-11-30T18:52:14.251252Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Agent 1: JD Analyst Agent\n# ============================================================================\n# Parses job descriptions into structured requirements\n\njd_analyst_agent = LlmAgent(\n    model=Gemini(model=MODEL_NAME_FAST, retry_options=retry_config),\n    name=\"jd_analyst_agent\",\n    description=\"Analyzes job descriptions and extracts structured requirements.\",\n    instruction=\"\"\"\nYou are an expert Job Description Analyst. Your role is to parse job postings \nand extract structured, actionable requirements.\n\nWhen given a job description:\n1. Use the extract_job_profile tool to get the schema\n2. Carefully read the JD and fill in ALL fields in the schema\n3. Be specific about skill requirements - distinguish must-haves from nice-to-haves\n4. Infer seniority level from experience requirements and responsibilities\n5. Note any hard constraints (location, visa, clearance requirements)\n\nOutput your analysis as a structured JSON object matching the schema.\nIf information is not present, use null rather than guessing.\n\nFocus on extracting information that will be useful for matching candidates.\n\"\"\",\n    tools=[extract_job_profile, parse_document],\n)\n\nprint(\"‚úÖ JD Analyst Agent created\")\nprint(f\"   Model: {MODEL_NAME_FAST}\")\nprint(f\"   Tools: extract_job_profile, parse_document\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T18:52:18.780739Z","iopub.execute_input":"2025-11-30T18:52:18.781055Z","iopub.status.idle":"2025-11-30T18:52:18.788091Z","shell.execute_reply.started":"2025-11-30T18:52:18.781032Z","shell.execute_reply":"2025-11-30T18:52:18.786973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Agent 2: Candidate Profiler Agent\n# ============================================================================\n# Extracts structured profiles from resumes\n\ncandidate_profiler_agent = LlmAgent(\n    model=Gemini(model=MODEL_NAME_FAST, retry_options=retry_config),\n    name=\"candidate_profiler_agent\",\n    description=\"Extracts structured candidate profiles from resumes.\",\n    instruction=\"\"\"\nYou are an expert Resume Analyst. Your role is to parse resumes and extract \nstructured candidate profiles.\n\nWhen given a resume:\n1. Use the extract_candidate_profile tool to get the schema\n2. Extract ALL fields from the resume content\n3. Calculate total years of experience from work history\n4. Identify primary skills (frequently used, core to their role) vs secondary\n5. Estimate seniority level based on titles, responsibilities, and experience\n6. Note any potential concerns (gaps, frequent job changes, etc.)\n\nOutput your analysis as a structured JSON object matching the schema.\nBe objective and factual - extract what's in the resume, don't embellish.\n\nIf parsing from URL, use parse_document first to get the text content.\n\"\"\",\n    tools=[extract_candidate_profile, parse_document],\n)\n\nprint(\"‚úÖ Candidate Profiler Agent created\")\nprint(f\"   Model: {MODEL_NAME_FAST}\")\nprint(f\"   Tools: extract_candidate_profile, parse_document\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T18:52:21.327673Z","iopub.execute_input":"2025-11-30T18:52:21.328247Z","iopub.status.idle":"2025-11-30T18:52:21.33472Z","shell.execute_reply.started":"2025-11-30T18:52:21.328222Z","shell.execute_reply":"2025-11-30T18:52:21.333601Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Agent 3: Fit Scorer Agent\n# ============================================================================\n# Computes match scores between job requirements and candidates\n\nfit_scorer_agent = LlmAgent(\n    model=Gemini(model=MODEL_NAME_STRONG, retry_options=retry_config),  # Use stronger model for reasoning\n    name=\"fit_scorer_agent\",\n    description=\"Evaluates candidate fit against job requirements and produces scores.\",\n    instruction=\"\"\"\nYou are an expert Hiring Evaluator. Your role is to objectively assess how well \na candidate matches a job's requirements.\n\nWhen given a job profile and candidate profile:\n1. Use the compute_fit_score tool to get the scoring rubric\n2. Score each dimension (skills, experience, domain, seniority, nice-to-haves)\n3. Calculate the weighted overall score\n4. Assign a fit label: Strong Fit (75+), Mixed Fit (50-74), Weak Fit (<50)\n5. Create an evidence table linking each job requirement to resume evidence\n6. List gaps (requirements not met) and strengths (exceeds requirements)\n7. Note any risks or concerns\n\nBe rigorous and fair:\n- Don't penalize for irrelevant missing skills\n- Do penalize for missing must-have requirements\n- Consider transferable skills and adjacent experience\n- Provide specific evidence for your scores\n\nOutput a structured evaluation with scores, evidence, and justification.\n\"\"\",\n    tools=[compute_fit_score],\n)\n\nprint(\"‚úÖ Fit Scorer Agent created\")\nprint(f\"   Model: {MODEL_NAME_STRONG} (stronger model for reasoning)\")\nprint(f\"   Tools: compute_fit_score\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T18:52:24.106665Z","iopub.execute_input":"2025-11-30T18:52:24.107001Z","iopub.status.idle":"2025-11-30T18:52:24.113806Z","shell.execute_reply.started":"2025-11-30T18:52:24.10698Z","shell.execute_reply":"2025-11-30T18:52:24.112697Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Agent 4: Report Writer Agent\n# ============================================================================\n# Generates final rankings and recruiter-ready summaries\n\nreport_writer_agent = LlmAgent(\n    model=Gemini(model=MODEL_NAME_FAST, retry_options=retry_config),\n    name=\"report_writer_agent\",\n    description=\"Generates ranked candidate shortlists and recruiter-ready summaries.\",\n    instruction=\"\"\"\nYou are an expert Hiring Report Writer. Your role is to synthesize candidate \nevaluations into clear, actionable reports for hiring managers.\n\nWhen given evaluated candidates:\n1. Rank candidates by overall fit score (highest first)\n2. For each candidate, provide:\n   - Quick verdict (1-2 sentences)\n   - Key strengths (bullet points)\n   - Key concerns (bullet points)\n   - Recommended interview focus areas\n3. Generate a summary comparison table\n4. Provide overall hiring recommendation\n\nFormat your output as:\n\n## üìä Candidate Ranking\n\n| Rank | Candidate | Score | Fit Label | Quick Verdict |\n|------|-----------|-------|-----------|---------------|\n\n## üìã Detailed Evaluations\n\n### 1. [Candidate Name] - [Score]/100 - [Fit Label]\n**Verdict:** ...\n**Strengths:** ...\n**Concerns:** ...\n**Interview Focus:** ...\n\n## üí° Hiring Recommendation\n...\n\nKeep language professional but accessible. Highlight actionable insights.\n\"\"\",\n    tools=[],  # No tools needed - pure synthesis\n)\n\nprint(\"‚úÖ Report Writer Agent created\")\nprint(f\"   Model: {MODEL_NAME_FAST}\")\nprint(f\"   Tools: None (synthesis only)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T18:52:26.956663Z","iopub.execute_input":"2025-11-30T18:52:26.957296Z","iopub.status.idle":"2025-11-30T18:52:26.963571Z","shell.execute_reply.started":"2025-11-30T18:52:26.957272Z","shell.execute_reply":"2025-11-30T18:52:26.962655Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Agent 5: Root Orchestrator with SequentialAgent Pipeline\n# ============================================================================\n# Multi-agent architecture using SequentialAgent for deterministic workflow\n# This demonstrates Day 1 concepts: Sequential, Parallel, and Loop agents\n# ============================================================================\n\nfrom google.adk.tools import AgentTool\n\n# --- Create a SequentialAgent for the screening pipeline ---\n# This ensures deterministic order: JD Analysis -> Candidate Profiling -> Scoring -> Report\n# The SequentialAgent runs each sub-agent in order, passing state between them\n\nscreening_pipeline = SequentialAgent(\n    name=\"screening_pipeline\",\n    sub_agents=[\n        jd_analyst_agent,\n        candidate_profiler_agent,\n        fit_scorer_agent,\n        report_writer_agent,\n    ],\n)\n\nprint(\"‚úÖ SequentialAgent pipeline created:\")\nprint(\"   1. jd_analyst_agent -> Analyze JD\")\nprint(\"   2. candidate_profiler_agent -> Profile candidates\")\nprint(\"   3. fit_scorer_agent -> Score fit\")\nprint(\"   4. report_writer_agent -> Generate report\")\n\n# --- Wrap sub-agents as tools for flexible orchestration ---\n# The root agent can use these tools for dynamic routing\njd_analyst_tool = AgentTool(agent=jd_analyst_agent)\ncandidate_profiler_tool = AgentTool(agent=candidate_profiler_agent)\nfit_scorer_tool = AgentTool(agent=fit_scorer_agent)\nreport_writer_tool = AgentTool(agent=report_writer_agent)\npipeline_tool = AgentTool(agent=screening_pipeline)\n\n# The root orchestrator agent\nagentic_recruiter = LlmAgent(\n    model=Gemini(model=MODEL_NAME_STRONG, retry_options=retry_config),\n    name=\"agentic_recruiter\",\n    description=\"\"\"\n    An intelligent recruiting assistant that helps hiring managers screen candidates.\n    Accepts job descriptions and resumes, then provides structured evaluations and rankings.\n    \"\"\",\n    instruction=\"\"\"\nYou are **Agentic Recruiter**, an AI-powered hiring assistant. You help hiring managers \nefficiently screen candidates by providing structured, fair, and actionable evaluations.\n\n## Your Workflow\n\n1. **Collect Input**\n   - Ask for the job description (text or URL)\n   - Ask for candidate resumes (text or URLs - can handle multiple)\n   - Ask for any specific preferences or constraints\n\n2. **Analyze Job Description**\n   - Use the jd_analyst_agent to parse the JD into structured requirements\n   - Confirm the extracted requirements with the hiring manager\n\n3. **Profile Candidates**\n   - Use the candidate_profiler_agent for EACH resume\n   - Extract structured profiles for all candidates\n\n4. **Score Candidates**\n   - Use the fit_scorer_agent to evaluate EACH candidate against the JD\n   - Produce detailed scores and evidence\n\n5. **Generate Report**\n   - Use the report_writer_agent to create the final ranking and summary\n   - Present results to the hiring manager\n\n## Alternative: Use the Pipeline\nFor a complete end-to-end screening, you can use the screening_pipeline tool\nwhich runs all agents in sequence automatically.\n\n## Guidelines\n\n- Be conversational and helpful\n- Ask clarifying questions when needed\n- Use memory (load_memory) to recall hiring manager preferences from past sessions\n- Always explain your reasoning\n- Be objective and avoid bias in evaluations\n- If something seems off, flag it for human review\n\n## Conversation Starters\n\nWhen a user starts a new session, you might say:\n\"Hello! I'm your AI recruiting assistant. I can help you screen candidates for a role.\nTo get started, please share:\n1. The job description (paste text or provide a URL)\n2. Candidate resumes (paste text, URLs, or upload files)\n\nI'll analyze the requirements, evaluate each candidate, and provide a ranked shortlist \nwith detailed explanations. Ready when you are!\"\n\"\"\",\n    tools=[\n        jd_analyst_tool,\n        candidate_profiler_tool,\n        fit_scorer_tool,\n        report_writer_tool,\n        pipeline_tool,  # Full sequential pipeline\n        parse_document,  # Direct document parsing\n        load_memory,     # Access long-term memory for preferences\n    ],\n)\n\nprint(\"\\n‚úÖ Agentic Recruiter (Root Orchestrator) created\")\nprint(f\"   Model: {MODEL_NAME_STRONG}\")\nprint(f\"   Sub-agents: jd_analyst, candidate_profiler, fit_scorer, report_writer\")\nprint(f\"   Pipeline: screening_pipeline (SequentialAgent)\")\nprint(f\"   Tools: parse_document, load_memory, pipeline_tool\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T18:52:29.802849Z","iopub.execute_input":"2025-11-30T18:52:29.803166Z","iopub.status.idle":"2025-11-30T18:52:29.813123Z","shell.execute_reply.started":"2025-11-30T18:52:29.803145Z","shell.execute_reply":"2025-11-30T18:52:29.812316Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create the App with Context Compaction, LoggingPlugin, and Runner\n# ============================================================================\n# Day 3: EventsCompactionConfig for context engineering\n# Day 4: LoggingPlugin for observability (passed to App, not Runner)\n# ============================================================================\n\n# --- Memory Callback for Long-term Memory ---\n# This callback saves session summaries to memory after each conversation\n\nasync def save_session_to_memory(callback_context):\n    \"\"\"\n    After-agent callback to save important session information to memory.\n    This enables long-term memory across sessions.\n    \"\"\"\n    # Get session state\n    state = callback_context.state\n\n    # Extract key information to remember\n    memory_items = []\n    if \"job_profile\" in state:\n        memory_items.append(f\"Job analyzed: {state.get('job_profile', {}).get('title', 'Unknown')}\")\n    if \"candidates_evaluated\" in state:\n        memory_items.append(f\"Candidates evaluated: {state.get('candidates_evaluated', 0)}\")\n    if \"hiring_preferences\" in state:\n        memory_items.append(f\"Hiring preferences: {state.get('hiring_preferences', '')}\")\n\n    # Log what we're saving\n    if memory_items:\n        print(f\"üíæ Saving to memory: {memory_items}\")\n\n    return None  # Continue normal flow\n\n# --- Create App with Context Compaction AND LoggingPlugin ---\n# IMPORTANT: When using App, plugins must be passed to App, not Runner!\n# EventsCompactionConfig automatically summarizes long conversations\n# LoggingPlugin provides observability into agent execution\nresume_matcher_app = App(\n    name=APP_NAME,\n    root_agent=agentic_recruiter,\n    events_compaction_config=EventsCompactionConfig(\n        compaction_interval=5,  # Compact after every 5 conversation turns\n        overlap_size=2,         # Keep 2 previous turns for context continuity\n    ),\n    plugins=[LoggingPlugin()],  # Day 4: Observability - must be in App!\n)\n\nprint(\"‚úÖ App created with:\")\nprint(f\"   - EventsCompactionConfig (compaction_interval=5, overlap_size=2)\")\nprint(f\"   - LoggingPlugin (observability enabled)\")\n\n# --- Create Runner ---\n# When using App, plugins are already configured in the App\nrunner = Runner(\n    app=resume_matcher_app,  # Use App instead of agent directly\n    session_service=session_service,\n    memory_service=memory_service,\n    # NOTE: plugins go in App, not here!\n)\n\nprint(\"\\n‚úÖ Runner created with:\")\nprint(f\"   App: {resume_matcher_app.name}\")\nprint(f\"   Session Service: {type(session_service).__name__}\")\nprint(f\"   Memory Service: {type(memory_service).__name__}\")\nprint(\"\\nüìä Logs will be written to resume_agent.log\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T18:52:33.457517Z","iopub.execute_input":"2025-11-30T18:52:33.457835Z","iopub.status.idle":"2025-11-30T18:52:33.468576Z","shell.execute_reply.started":"2025-11-30T18:52:33.457814Z","shell.execute_reply":"2025-11-30T18:52:33.467628Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Helper function to run a conversation turn\n# ============================================================================\n\nasync def chat(user_message: str, session_id: str = \"default\"):\n    \"\"\"\n    Send a message to the Agentic Recruiter and display the response.\n    \n    Args:\n        user_message: The message from the hiring manager\n        session_id: Session identifier for conversation continuity\n    \"\"\"\n    # Create or retrieve session\n    try:\n        session = await session_service.create_session(\n            app_name=APP_NAME, user_id=USER_ID, session_id=session_id\n        )\n    except:\n        session = await session_service.get_session(\n            app_name=APP_NAME, user_id=USER_ID, session_id=session_id\n        )\n    \n    # Create the message content\n    message_content = types.Content(\n        role=\"user\",\n        parts=[types.Part(text=user_message)]\n    )\n    \n    print(f\"\\nüë§ Hiring Manager: {user_message[:200]}{'...' if len(user_message) > 200 else ''}\")\n    print(\"\\nü§ñ Agentic Recruiter:\")\n    print(\"-\" * 60)\n    \n    # Run the agent and stream response\n    async for event in runner.run_async(\n        user_id=USER_ID,\n        session_id=session.id,\n        new_message=message_content\n    ):\n        if event.is_final_response() and event.content and event.content.parts:\n            for part in event.content.parts:\n                if hasattr(part, 'text') and part.text:\n                    print(part.text)\n    \n    print(\"-\" * 60)\n\n\nprint(\"‚úÖ Chat helper function ready\")\nprint(\"   Usage: await chat('Your message here', session_id='session_name')\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T18:52:36.863843Z","iopub.execute_input":"2025-11-30T18:52:36.864813Z","iopub.status.idle":"2025-11-30T18:52:36.872951Z","shell.execute_reply.started":"2025-11-30T18:52:36.864781Z","shell.execute_reply":"2025-11-30T18:52:36.871875Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!adk web --log_level DEBUG --url_prefix {url_prefix}\nprint(\"SKIPPING ADK WEB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T18:53:41.411931Z","iopub.execute_input":"2025-11-30T18:53:41.412268Z","iopub.status.idle":"2025-11-30T18:53:58.737382Z","shell.execute_reply.started":"2025-11-30T18:53:41.412236Z","shell.execute_reply":"2025-11-30T18:53:58.736285Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test the system with a sample job description and resume\n# ============================================================================\n\n# Sample Job Description\nSAMPLE_JD = \"\"\"\nSenior Machine Learning Engineer - Fintech\n\nAbout the Role:\nWe're looking for a Senior ML Engineer to join our fraud detection team. You'll build \nand deploy real-time ML models that protect millions of transactions daily.\n\nRequirements:\n- 5+ years of experience in machine learning or data science\n- Strong Python skills (must have)\n- Experience with deep learning frameworks (PyTorch or TensorFlow)\n- Experience with real-time ML systems and streaming data\n- Background in fraud detection or financial services (preferred)\n- Experience with Kubernetes and cloud platforms (AWS/GCP)\n\nNice to have:\n- PhD in ML, Statistics, or related field\n- Experience with graph neural networks\n- Contributions to open source ML projects\n\nLocation: San Francisco (Hybrid - 3 days in office)\nSalary: $180,000 - $250,000 + equity\n\"\"\"\n\n# Sample Resume 1 - Strong Fit\nSAMPLE_RESUME_1 = \"\"\"\nJANE SMITH\nSenior Machine Learning Engineer\nSan Francisco, CA | jane.smith@email.com\n\nEXPERIENCE:\n\nSenior ML Engineer - PaymentGuard Inc. (2021-Present)\n- Built real-time fraud detection system processing 10M+ transactions/day\n- Reduced fraud losses by 40% using ensemble deep learning models\n- Deployed models on Kubernetes with sub-100ms latency requirements\n- Tech: Python, PyTorch, Kafka, Kubernetes, AWS\n\nML Engineer - DataCorp (2018-2021)\n- Developed credit scoring models for fintech clients\n- Built MLOps pipelines for model training and deployment\n- Tech: Python, TensorFlow, scikit-learn, GCP\n\nData Scientist - Analytics Co (2016-2018)\n- Built predictive models for customer churn\n- Tech: Python, R, SQL\n\nEDUCATION:\nMS Computer Science (ML Focus) - Stanford University, 2016\nBS Mathematics - UC Berkeley, 2014\n\nSKILLS:\nPython, PyTorch, TensorFlow, Kubernetes, AWS, GCP, Kafka, SQL, MLOps\n\"\"\"\n\n# Sample Resume 2 - Mixed Fit\nSAMPLE_RESUME_2 = \"\"\"\nJOHN DOE\nData Scientist\nAustin, TX | john.doe@email.com\n\nEXPERIENCE:\n\nSenior Data Scientist - E-commerce Corp (2020-Present)\n- Built recommendation systems for product discovery\n- Developed A/B testing framework for ML experiments\n- Tech: Python, TensorFlow, AWS SageMaker\n\nData Scientist - Marketing Analytics (2017-2020)\n- Built customer segmentation models\n- Created dashboards for marketing performance\n- Tech: Python, scikit-learn, SQL, Tableau\n\nEDUCATION:\nMS Data Science - UT Austin, 2017\nBS Statistics - Texas A&M, 2015\n\nSKILLS:\nPython, TensorFlow, scikit-learn, SQL, AWS, Tableau, A/B Testing\n\"\"\"\n\nprint(\"üìù Sample data ready for testing\")\nprint(f\"   JD: Senior ML Engineer - Fintech ({len(SAMPLE_JD)} chars)\")\nprint(f\"   Resume 1: Jane Smith - Strong fit ({len(SAMPLE_RESUME_1)} chars)\")\nprint(f\"   Resume 2: John Doe - Mixed fit ({len(SAMPLE_RESUME_2)} chars)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T18:58:52.746639Z","iopub.execute_input":"2025-11-30T18:58:52.747706Z","iopub.status.idle":"2025-11-30T18:58:52.75443Z","shell.execute_reply.started":"2025-11-30T18:58:52.747672Z","shell.execute_reply":"2025-11-30T18:58:52.753468Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run a test screening session\n# ============================================================================\n\n# Start the conversation\nawait chat(\"Hello! I need help screening candidates for a role.\", session_id=\"test_screening\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T18:58:56.258712Z","iopub.execute_input":"2025-11-30T18:58:56.259748Z","iopub.status.idle":"2025-11-30T18:58:57.686883Z","shell.execute_reply.started":"2025-11-30T18:58:56.259713Z","shell.execute_reply":"2025-11-30T18:58:57.685889Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Provide the job description\nawait chat(f\"Here's the job description:\\n\\n{SAMPLE_JD}\", session_id=\"test_screening\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T18:59:03.355581Z","iopub.execute_input":"2025-11-30T18:59:03.355869Z","iopub.status.idle":"2025-11-30T18:59:06.59519Z","shell.execute_reply.started":"2025-11-30T18:59:03.355849Z","shell.execute_reply":"2025-11-30T18:59:06.593856Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Provide the first resume\nawait chat(f\"Here's the first candidate resume:\\n\\n{SAMPLE_RESUME_1}\", session_id=\"test_screening\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T18:59:14.489535Z","iopub.execute_input":"2025-11-30T18:59:14.490307Z","iopub.status.idle":"2025-11-30T18:59:18.362882Z","shell.execute_reply.started":"2025-11-30T18:59:14.490284Z","shell.execute_reply":"2025-11-30T18:59:18.361571Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Provide the second resume\nawait chat(f\"Here's the second candidate:\\n\\n{SAMPLE_RESUME_2}\", session_id=\"test_screening\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T18:59:27.515413Z","iopub.execute_input":"2025-11-30T18:59:27.516338Z","iopub.status.idle":"2025-11-30T18:59:30.228359Z","shell.execute_reply.started":"2025-11-30T18:59:27.51631Z","shell.execute_reply":"2025-11-30T18:59:30.227047Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ask for the final ranking\nawait chat(\"Please provide the final ranking and recommendations for these candidates.\", session_id=\"test_screening\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T18:59:35.595649Z","iopub.execute_input":"2025-11-30T18:59:35.596258Z","iopub.status.idle":"2025-11-30T19:00:39.162901Z","shell.execute_reply.started":"2025-11-30T18:59:35.596231Z","shell.execute_reply":"2025-11-30T19:00:39.16203Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the current session to memory for future recall\n# ============================================================================\n\nasync def save_session_to_memory(session_id: str):\n    \"\"\"Save a session to long-term memory.\"\"\"\n    session = await session_service.get_session(\n        app_name=APP_NAME, user_id=USER_ID, session_id=session_id\n    )\n    await memory_service.add_session_to_memory(session)\n    print(f\"‚úÖ Session '{session_id}' saved to memory\")\n\n# Save our test session\nawait save_session_to_memory(\"test_screening\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Agent Evaluation Setup (Day 4)\n# ============================================================================\n# Create evaluation test cases and configuration for `adk eval`\n# This demonstrates systematic agent testing and quality measurement\n# ============================================================================\n\nimport json\n\n# --- Evaluation Test Cases (evalset.json) ---\n# These test cases verify the agent's core capabilities\neval_set = {\n    \"eval_set_id\": \"resume_screener_eval\",\n    \"name\": \"Resume Screener Evaluation\",\n    \"description\": \"Test cases for the multi-agent resume screening system\",\n    \"test_cases\": [\n        {\n            \"test_case_id\": \"tc_greeting\",\n            \"name\": \"Greeting Response\",\n            \"description\": \"Agent should greet and explain its capabilities\",\n            \"conversation\": [\n                {\n                    \"role\": \"user\",\n                    \"content\": \"Hello, I need help screening candidates.\"\n                }\n            ],\n            \"expected_behaviors\": [\n                \"Agent introduces itself as a recruiting assistant\",\n                \"Agent asks for job description\",\n                \"Agent mentions it can analyze resumes\"\n            ]\n        },\n        {\n            \"test_case_id\": \"tc_jd_analysis\",\n            \"name\": \"Job Description Analysis\",\n            \"description\": \"Agent should extract structured requirements from JD\",\n            \"conversation\": [\n                {\n                    \"role\": \"user\",\n                    \"content\": \"Here is the JD: Senior Python Developer, 5+ years experience, must know Django and PostgreSQL, nice to have: AWS, Docker.\"\n                }\n            ],\n            \"expected_behaviors\": [\n                \"Agent identifies required skills (Python, Django, PostgreSQL)\",\n                \"Agent identifies nice-to-have skills (AWS, Docker)\",\n                \"Agent identifies experience requirement (5+ years)\"\n            ]\n        },\n        {\n            \"test_case_id\": \"tc_candidate_scoring\",\n            \"name\": \"Candidate Scoring\",\n            \"description\": \"Agent should provide objective fit scores\",\n            \"conversation\": [\n                {\n                    \"role\": \"user\",\n                    \"content\": \"JD: Python Developer, 3+ years. Resume: John Doe, 5 years Python, Django expert, AWS certified.\"\n                }\n            ],\n            \"expected_behaviors\": [\n                \"Agent provides a numerical fit score\",\n                \"Agent explains strengths (exceeds experience, has Django)\",\n                \"Agent provides objective assessment\"\n            ]\n        }\n    ]\n}\n\n# --- Evaluation Configuration (test_config.json) ---\n# Defines how to evaluate agent responses\neval_config = {\n    \"evaluation_criteria\": [\n        {\n            \"name\": \"relevance\",\n            \"description\": \"Response is relevant to the user's request\",\n            \"weight\": 0.3\n        },\n        {\n            \"name\": \"accuracy\",\n            \"description\": \"Information extracted is accurate and complete\",\n            \"weight\": 0.3\n        },\n        {\n            \"name\": \"helpfulness\",\n            \"description\": \"Response provides actionable guidance\",\n            \"weight\": 0.2\n        },\n        {\n            \"name\": \"professionalism\",\n            \"description\": \"Response maintains professional tone\",\n            \"weight\": 0.2\n        }\n    ],\n    \"model\": \"gemini-2.5-flash\",\n    \"threshold\": 0.7\n}\n\n# Save evaluation files\nwith open(\"evalset.json\", \"w\") as f:\n    json.dump(eval_set, f, indent=2)\n\nwith open(\"test_config.json\", \"w\") as f:\n    json.dump(eval_config, f, indent=2)\n\nprint(\"‚úÖ Evaluation files created:\")\nprint(\"   - evalset.json (3 test cases)\")\nprint(\"   - test_config.json (4 evaluation criteria)\")\nprint(\"\\nüìù To run evaluation locally:\")\nprint(\"   !adk eval resume_agent evalset.json --config_file_path=test_config.json\")\nprint(\"\\nüí° For production, use Vertex AI Evaluation:\")\nprint(\"   from vertexai.evaluation import EvalTask\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test memory recall in a new session\n# ============================================================================\n\nawait chat(\n    \"I'm back! Can you remind me what role we were screening for last time?\", \n    session_id=\"memory_test\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Agent Deployment to Vertex AI Agent Engine (Day 5)\n# ============================================================================\n# This section shows how to deploy the agent to production\n# Agent Engine provides: auto-scaling, session management, Memory Bank\n# ============================================================================\n\nprint(\"üöÄ DEPLOYMENT GUIDE: Vertex AI Agent Engine -- UNCOMMENT\")\n# print(\"=\" * 60)\n\n# print(\"\"\"\n# ## Step 1: Create Agent Directory Structure\n\n# Your agent code should be organized as:\n\n# resume_agent/\n# ‚îú‚îÄ‚îÄ agent.py              # Main agent definition\n# ‚îú‚îÄ‚îÄ requirements.txt      # Python dependencies\n# ‚îú‚îÄ‚îÄ .env                  # Environment variables\n# ‚îî‚îÄ‚îÄ .agent_engine_config.json  # Deployment config\n\n# ## Step 2: Create requirements.txt\n# \"\"\")\n\n# # Create requirements.txt content\n# requirements_content = \"\"\"google-adk>=0.1.0\n# google-cloud-aiplatform\n# PyPDF2\n# python-docx\n# requests\n# \"\"\"\n\n# print(\"üìÑ requirements.txt:\")\n# print(requirements_content)\n\n# # Create .env content\n# env_content = \"\"\"# Environment variables for Agent Engine\n# GOOGLE_CLOUD_LOCATION=global\n# GOOGLE_GENAI_USE_VERTEXAI=1\n# \"\"\"\n\n# print(\"üìÑ .env:\")\n# print(env_content)\n\n# # Create deployment config\n# deploy_config = {\n#     \"min_instances\": 0,\n#     \"max_instances\": 2,\n#     \"cpu\": \"1\",\n#     \"memory\": \"2Gi\"\n# }\n\n# print(\"üìÑ .agent_engine_config.json:\")\n# print(json.dumps(deploy_config, indent=2))\n\n# print(\"\"\"\n# ## Step 3: Deploy with ADK CLI\n\n# Run this command to deploy:\n\n# ```bash\n# adk deploy agent_engine \\\n#     --project=$PROJECT_ID \\\n#     --region=us-central1 \\\n#     resume_agent/ \\\n#     --agent_engine_config_file=resume_agent/.agent_engine_config.json\n# ```\n\n# ## Step 4: Test Deployed Agent\n\n# ```python\n# import vertexai\n# from vertexai import agent_engines\n\n# vertexai.init(project=PROJECT_ID, location=\"us-central1\")\n\n# # Get deployed agent\n# agents_list = list(agent_engines.list())\n# remote_agent = agents_list[0]\n\n# # Query the agent\n# async for item in remote_agent.async_stream_query(\n#     message=\"Hello, I need help screening candidates.\",\n#     user_id=\"user_123\",\n# ):\n#     print(item)\n# ```\n\n# ## Step 5: Memory Bank (Automatic with Agent Engine!)\n\n# When deployed to Agent Engine, Memory Bank is automatically enabled:\n# - Long-term memory across sessions\n# - LLM-powered memory consolidation\n# - Semantic search for relevant memories\n\n# ## Step 6: Cleanup (Important!)\n\n# ```python\n# # Delete agent to avoid charges\n# agent_engines.delete(resource_name=remote_agent.resource_name, force=True)\n# ```\n# \"\"\")\n\n# print(\"\\n‚úÖ Deployment guide complete!\")\n# print(\"   See Day 5 notebook for full deployment walkthrough\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}